import argparse
import os

import gradio as gr
import yaml

from common.log import logger
from common.subprocess_utils import run_script_with_log

from modelscope.pipelines import pipeline
from modelscope.utils.constant import Tasks
from videoclipper import VideoClipper
import librosa
import soundfile as sf
import numpy as np
import random

dataset_root = ".\\raw\\"





# å­—å¹•è¯­éŸ³åˆ‡åˆ†
inference_pipeline = pipeline(
    task=Tasks.auto_speech_recognition,
    model='damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch',
    vad_model='damo/speech_fsmn_vad_zh-cn-16k-common-pytorch',
    punc_model='damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch',
    ncpu=16,
)
sd_pipeline = pipeline(
    task='speaker-diarization',
    model='damo/speech_campplus_speaker-diarization_common',
    model_revision='v1.0.0'
)
audio_clipper = VideoClipper(inference_pipeline, sd_pipeline)

def audio_change(audio):

    print(audio)

    sf.write('./output_44100.wav', audio[1], audio[0], 'PCM_24')

    y, sr = librosa.load('./output_44100.wav', sr=16000)

    # sf.write('./output_16000.wav', y, sr, 'PCM_24')

    # arr = np.array(y, dtype=np.int32)

    # y, sr = librosa.load('./output_16000.wav', sr=16000)

    audio_data = np.array(y)

    print(y, sr)

    return (16000,audio_data)

def write_list(text,audio):
    
    random_number = random.randint(10000, 99999)

    wav_name = f'./wavs/sample_{random_number}.wav'

    sf.write(wav_name, audio[1], audio[0], 'PCM_24')

    text = text.replace("#",",")

    with open("./esd.list","a",encoding="utf-8")as f:f.write(f"\n{wav_name}|sample|en|{text}")




def audio_recog(audio_input, sd_switch):
    print(audio_input)
    return audio_clipper.recog(audio_input, sd_switch)

def audio_clip(dest_text, audio_spk_input, start_ost, end_ost, state):
    return audio_clipper.clip(dest_text, start_ost, end_ost, state, dest_spk=audio_spk_input)

# éŸ³é¢‘é™å™ª

def reset_tts_wav(audio):

    ans = pipeline(
    Tasks.acoustic_noise_suppression,
    model='damo/speech_frcrn_ans_cirm_16k')
    ans(audio,output_path='./output_ins.wav')

    return "./output_ins.wav","./output_ins.wav"


def do_slice(
    dataset_path: str,
    min_sec: int,
    max_sec: int,
    min_silence_dur_ms: int,
):
    if dataset_path == "":
        return "Error: æ•°æ®é›†è·¯å¾„ä¸èƒ½ä¸ºç©º"
    logger.info("Start slicing...")
    output_dir = os.path.join(dataset_root, dataset_path, ".\\wavs")


    cmd = [
        "audio_slicer_pre.py",
        "--dataset_path",
        dataset_path,
        "--min_sec",
        str(min_sec),
        "--max_sec",
        str(max_sec),
        "--min_silence_dur_ms",
        str(min_silence_dur_ms),
    ]
    

    success, message = run_script_with_log(cmd, ignore_warning=True)
    if not success:
        return f"Error: {message}"
    return "åˆ‡åˆ†å®Œæ¯•"


def do_transcribe_fwhisper(
    model_name,mytype,language,input_file,file_pos
):
    # if model_name == "":
    #     return "Error: è§’è‰²åä¸èƒ½ä¸ºç©º"
    
    
    cmd_py = "short_audio_transcribe_fwhisper.py"


    success, message = run_script_with_log(
        [
            cmd_py,
            "--model_name",
            model_name,
            "--language",
            language,
            "--mytype",
            mytype,"--input_file",
            input_file,
            "--file_pos",
            file_pos,

        ]
    )
    if not success:
        return f"Error: {message}"
    return "è½¬å†™å®Œæ¯•"

def do_transcribe_whisper(
    model_name,mytype,language,input_file,file_pos
):
    # if model_name == "":
    #     return "Error: è§’è‰²åä¸èƒ½ä¸ºç©º"
    
    
    cmd_py = "short_audio_transcribe_whisper.py"


    success, message = run_script_with_log(
        [
            cmd_py,
            "--model_name",
            model_name,
            "--language",
            language,
            "--mytype",
            mytype,"--input_file",
            input_file,
            "--file_pos",
            file_pos,

        ]
    )
    if not success:
        return f"Error: {message}"
    return "è½¬å†™å®Œæ¯•"


def do_transcribe_all(
    model_name,mytype,language,input_file,file_pos
):
    # if model_name == "":
    #     return "Error: è§’è‰²åä¸èƒ½ä¸ºç©º"
    

    cmd_py = "short_audio_transcribe_ali.py"


    if mytype == "bcut":

        cmd_py = "short_audio_transcribe_bcut.py"

    success, message = run_script_with_log(
        [
            cmd_py,
            "--model_name",
            model_name,
            "--language",
            language,
            "--input_file",
            input_file,
            "--file_pos",
            file_pos,

        ]
    )
    if not success:
        return f"Error: {message}"
    return "è½¬å†™å®Œæ¯•"


initial_md = """

è¯·æŠŠæ ¼å¼ä¸º è§’è‰²å.wav çš„ç´ ææ–‡ä»¶æ”¾å…¥é¡¹ç›®çš„rawç›®å½•

ä½œè€…ï¼šåˆ˜æ‚¦çš„æŠ€æœ¯åšå®¢  https://space.bilibili.com/3031494

"""

with gr.Blocks(theme="NoCrypt/miku") as app:
    gr.Markdown(initial_md)
    model_name = gr.Textbox(label="è§’è‰²å",placeholder="è¯·è¾“å…¥è§’è‰²å",visible=False)


    with gr.Accordion("å¹²å£°æŠ½ç¦»å’Œé™å™ª"):
        with gr.Row():
            audio_inp_path = gr.Audio(label="è¯·ä¸Šä¼ å…‹éš†å¯¹è±¡éŸ³é¢‘", type="filepath")
            reset_inp_button = gr.Button("é’ˆå¯¹åŸå§‹ç´ æè¿›è¡Œé™å™ª", variant="primary",visible=True)
            reset_dataset_path = gr.Textbox(label="é™å™ªåéŸ³é¢‘åœ°å€",placeholder="é™å™ªåç”Ÿæˆçš„éŸ³é¢‘åœ°å€")

        
    reset_inp_button.click(reset_tts_wav,[audio_inp_path],[audio_inp_path,reset_dataset_path])
    
    with gr.Accordion("éŸ³é¢‘ç´ æåˆ‡å‰²"):
        with gr.Row():
            ##add by hyh æ·»åŠ ä¸€ä¸ªæ•°æ®é›†è·¯å¾„çš„æ–‡æœ¬æ¡†
            dataset_path = gr.Textbox(label="éŸ³é¢‘ç´ ææ‰€åœ¨è·¯å¾„ï¼Œé»˜è®¤åœ¨é¡¹ç›®çš„rawæ–‡ä»¶å¤¹,æ”¯æŒæ‰¹é‡è§’è‰²åˆ‡åˆ†",placeholder="è®¾ç½®éŸ³é¢‘ç´ ææ‰€åœ¨è·¯å¾„",value="./raw/")
            with gr.Column():
                
                min_sec = gr.Slider(
                    minimum=0, maximum=7000, value=2500, step=100, label="æœ€ä½å‡ æ¯«ç§’"
                )
                max_sec = gr.Slider(
                    minimum=0, maximum=15000, value=5000, step=100, label="æœ€é«˜å‡ æ¯«ç§’"
                )
                min_silence_dur_ms = gr.Slider(
                    minimum=500,
                    maximum=5000,
                    value=500,
                    step=100,
                    label="max_sil_kepté•¿åº¦",
                )
                slice_button = gr.Button("å¼€å§‹åˆ‡åˆ†")
            result1 = gr.Textbox(label="çµæœ")

    with gr.Accordion("éŸ³é¢‘ç´ ææ‰‹åŠ¨æŒ‰å­—å¹•åˆ‡å‰²"):
        audio_state = gr.State()
        with gr.Row():
            with gr.Column():
                # oaudio_input = gr.Audio(label="ğŸ”ŠéŸ³é¢‘è¾“å…¥ 44100hz Audio Input",type="filepath")
                # rec_audio = gr.Button("ğŸ‘‚é‡æ–°é‡‡æ ·")
                audio_input = gr.Audio(label="ğŸ”ŠéŸ³é¢‘è¾“å…¥ 16000hz Audio Input")
                audio_sd_switch = gr.Radio(["no", "yes"], label="ğŸ‘¥æ˜¯å¦åŒºåˆ†è¯´è¯äºº Recognize Speakers", value='no')
                recog_button1 = gr.Button("ğŸ‘‚è¯†åˆ« Recognize")
                audio_text_output = gr.Textbox(label="âœï¸è¯†åˆ«ç»“æœ Recognition Result")
                audio_srt_output = gr.Textbox(label="ğŸ“–SRTå­—å¹•å†…å®¹ RST Subtitles")
            with gr.Column():
                audio_text_input = gr.Textbox(label="âœï¸å¾…è£å‰ªæ–‡æœ¬ Text to Clip (å¤šæ®µæ–‡æœ¬ä½¿ç”¨'#'è¿æ¥)")
                audio_spk_input = gr.Textbox(label="âœï¸å¾…è£å‰ªè¯´è¯äºº Speaker to Clip (å¤šä¸ªè¯´è¯äººä½¿ç”¨'#'è¿æ¥)")
                with gr.Row():
                    audio_start_ost = gr.Slider(minimum=-500, maximum=1000, value=0, step=50, label="âªå¼€å§‹ä½ç½®åç§» Start Offset (ms)")
                    audio_end_ost = gr.Slider(minimum=-500, maximum=1000, value=0, step=50, label="â©ç»“æŸä½ç½®åç§» End Offset (ms)")
                with gr.Row():
                    clip_button1 = gr.Button("âœ‚ï¸è£å‰ª Clip")
                    write_button1 = gr.Button("å†™å…¥è½¬å†™æ–‡ä»¶")
                audio_output = gr.Audio(label="ğŸ”Šè£å‰ªç»“æœ Audio Clipped")
                audio_mess_output = gr.Textbox(label="â„¹ï¸è£å‰ªä¿¡æ¯ Clipping Log")
                audio_srt_clip_output = gr.Textbox(label="ğŸ“–è£å‰ªéƒ¨åˆ†SRTå­—å¹•å†…å®¹ Clipped RST Subtitles")

            audio_input.change(inputs=audio_input, outputs=audio_input, fn=audio_change)

            write_button1.click(write_list,[audio_text_input,audio_output],[])
            
            # rec_audio.click(re_write,[oaudio_input],[rec_audio])
            recog_button1.click(audio_recog, 
                            inputs=[audio_input, audio_sd_switch],
                            outputs=[audio_text_output, audio_srt_output, audio_state])
            clip_button1.click(audio_clip, 
                            inputs=[audio_text_input, audio_spk_input, audio_start_ost, audio_end_ost, audio_state], 
                            outputs=[audio_output, audio_mess_output, audio_srt_clip_output])



    with gr.Row():
        with gr.Column():
            
            language = gr.Dropdown(["ja", "en", "zh"], value="zh", label="é€‰æ‹©è½¬å†™çš„è¯­è¨€")

            mytype = gr.Dropdown(["small","medium","large-v3","large-v2"], value="medium", label="é€‰æ‹©Whisperæ¨¡å‹")

            input_file = gr.Textbox(label="åˆ‡ç‰‡æ‰€åœ¨ç›®å½•",placeholder="ä¸å¡«é»˜è®¤ä¸º./wavsç›®å½•")
            
            file_pos = gr.Textbox(label="åˆ‡ç‰‡åç§°å‰ç¼€",placeholder="ä¸å¡«åªæœ‰åˆ‡ç‰‡æ–‡ä»¶å")
            
        transcribe_button_whisper = gr.Button("Whisperå¼€å§‹è½¬å†™")

        transcribe_button_fwhisper = gr.Button("Faster-Whisperå¼€å§‹è½¬å†™")

        transcribe_button_ali = gr.Button("é˜¿é‡ŒASRå¼€å§‹è½¬å†™")

        transcribe_button_bcut = gr.Button("å¿…å‰ªASRå¼€å§‹è½¬å†™")


        result2 = gr.Textbox(label="çµæœ")

    slice_button.click(
        do_slice,
        inputs=[dataset_path, min_sec, max_sec, min_silence_dur_ms],
        outputs=[result1],
    )
    transcribe_button_whisper.click(
        do_transcribe_whisper,
        inputs=[
            model_name,
            mytype,
            language,input_file,file_pos
        ],
        outputs=[result2],)


    transcribe_button_fwhisper.click(
        do_transcribe_fwhisper,
        inputs=[
            model_name,
            mytype,
            language,input_file,file_pos
        ],
        outputs=[result2],)


    ali = gr.Text(value="ali",visible=False)

    bcut = gr.Text(value="bcut",visible=False)


    transcribe_button_ali.click(
        do_transcribe_all,
        inputs=[
            model_name,
            ali,
            language,input_file,file_pos
        ],
        outputs=[result2],
    )

    transcribe_button_bcut.click(
        do_transcribe_all,
        inputs=[
            model_name,
            bcut,
            language,input_file,file_pos
        ],
        outputs=[result2],
    )

parser = argparse.ArgumentParser()
parser.add_argument(
    "--server-name",
    type=str,
    default=None,
    help="Server name for Gradio app",
)
parser.add_argument(
    "--no-autolaunch",
    action="store_true",
    default=False,
    help="Do not launch app automatically",
)
args = parser.parse_args()

app.launch(inbrowser=not args.no_autolaunch, server_name=args.server_name, server_port=7971)
